<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GenAI Lab Programs</title>
    <style>
        /* Dark Mode Theme - Deeper Dark */
:root {
    --bg-color: #0d1117;
    --text-color: #e6edf3;
    --header-bg: #161b22;
    --container-bg: #21262d;
    --accent-color: #58a6ff;
    --border-color: #30363d;
    --code-bg: #0d1117;
    --copy-btn-bg: #21262d;
    --copy-btn-hover: #30363d;
}

* {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
    height: 100vh;
    margin: 0;
    padding: 0;
}

.container {
    height: 100vh;
    display: flex;
    flex-direction: column;
}

.program-selection {
    display: flex;
    justify-content: space-between;
    align-items: center;
    background-color: var(--header-bg);
    padding: 10px 20px;
    border-bottom: 1px solid var(--border-color);
}

select {
    padding: 8px;
    border-radius: 4px;
    border: 1px solid var(--border-color);
    background-color: var(--header-bg);
    color: var(--text-color);
    font-size: 14px;
    cursor: pointer;
    flex: 1;
    max-width: 400px;
}

select:focus {
    outline: 1px solid var(--accent-color);
}

#program-indicator {
    font-size: 14px;
    color: var(--accent-color);
    font-weight: 500;
}

.copy-btn {
    background-color: var(--copy-btn-bg);
    color: var(--text-color);
    border: 1px solid var(--border-color);
    border-radius: 4px;
    width: 36px;
    height: 36px;
    font-size: 16px;
    cursor: pointer;
    transition: background-color 0.2s;
    display: flex;
    align-items: center;
    justify-content: center;
}

.copy-btn:hover {
    background-color: var(--copy-btn-hover);
}

.copy-btn.copied {
    background-color: #238636;
}

main {
    flex: 1;
    overflow: auto;
    background-color: var(--bg-color);
    padding: 0;
}

#program-content {
    width: 100%;
    height: 100%;
    margin: 0;
    white-space: pre-wrap;
    font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
    line-height: 1.5;
    background-color: var(--code-bg);
    padding: 20px;
    border: none;
    color: var(--text-color);
    overflow: auto;
    box-sizing: border-box;
    font-size: 15px;
    cursor: pointer;
    user-select: text;
    transition: background-color 0.2s;
}

#program-content:hover {
    background-color: #141a21;
}

/* Responsive adjustments */
@media (max-width: 768px) {
    .program-selection {
        padding: 8px 12px;
    }
    
    #program-indicator {
        font-size: 12px;
    }
    
    .copy-btn {
        width: 32px;
        height: 32px;
        font-size: 14px;
    }
} 
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <div class="container">
        <div class="program-selection">
            <div id="program-indicator">Program 1/9 (V1)</div>
            <button id="copy-btn" class="copy-btn" title="Copy Code"><i class="fas fa-copy"></i></button>
        </div>
        
        <main>
            <pre id="program-content" title="Double-click to toggle between versions">// Select a program to view its code</pre>
        </main>
    </div>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
    // DOM Elements
    const programIndicator = document.getElementById('program-indicator');
    const programContent = document.getElementById('program-content');
    const copyBtn = document.getElementById('copy-btn');
    
    // Store original code content for clean copying
    let rawCodeContent = '';
    
    // Current program tracking
    let currentProgramIndex = 0;
    const totalPrograms = 9;
    // Track current version (1 or 2)
    let currentVersion = 1;
    
    // Touch tracking for swipe detection
    let touchStartX = 0;
    let touchEndX = 0;
    
    // Program data
    const programs = {
        program1: {
            title: "Program 1: Word Embeddings with GloVe",
            v1: {
                content: `# Word Embeddings with GloVe (Version 1)

This program demonstrates the use of pre-trained GloVe word embeddings for word relationship analysis.

\`\`\`python
from gensim.downloader import load

# Load the pre-trained GloVe model (50 dimensions)
print("Loading pre-trained GloVe model (50 dimensions)...")
model = load("glove-wiki-gigaword-50")

# Function to perform vector arithmetic and analyze relationships
def ewr():
    result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)
    print("\\nking - man + woman = ?", result[0][0])
    print("similarity:", result[0][1])
    
    result = model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=1)
    print("\\nparis - france + italy = ?", result[0][0])
    print("similarity:", result[0][1])
    
    # Example 3: Find analogies for programming
    result = model.most_similar(positive=['programming'], topn=5)
    print("\\nTop 5 words similar to 'programming':")
    for word, similarity in result:
        print(word, similarity)
    print(result)

ewr()
\`\`\``
            },
            v2: {
                content: `# Word Embeddings with GloVe (Version 2)

This program demonstrates an alternative approach to using pre-trained GloVe word embeddings.

\`\`\`python
double click
\`\`\``
            }
        },
        program2: {
            title: "Program 2: Word Embeddings Visualization with PCA",
            v1: {
                content: `# Word Embeddings Visualization with PCA (Version 1)

This program demonstrates dimensionality reduction and visualization of word embeddings using PCA.

\`\`\`python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from gensim.downloader import load

# Dimensionality reduction using PCA
def rd(ems):
    pca = PCA(n_components=2)
    r = pca.fit_transform(ems)
    return r

# Visualize word embeddings
def visualize(words, ems):
    plt.figure(figsize=(10, 6))
    for i, word in enumerate(words):
        x, y = ems[i]
        plt.scatter(x, y, marker='o', color='blue')
        plt.text(x + 0.02, y + 0.02, word, fontsize=12)
    plt.show()

# Generate semantically similar words
def gsm(word):
    sw = model.most_similar(word, topn=5)
    for word, s in sw:
        print(word, s)

# Load pre-trained GloVe model from Gensim API
print("Loading pre-trained GloVe model (50 dimensions)...")
model = load("glove-wiki-gigaword-50")

words = ['football', 'basketball', 'soccer', 'tennis', 'cricket']
ems = [model[word] for word in words]
e = rd(ems)
visualize(words, e)
gsm("programming")
\`\`\``
            },
            v2: {
                content: `# Word Embeddings Visualization with PCA (Version 2)

This program demonstrates an alternative approach to dimensionality reduction and visualization.

\`\`\`python
from gensim.downloader import load
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

model=load("glove-wiki-gigaword-50")
words = ['football', 'basketball', 'soccer', 'tennis', 'cricket']
vecs=PCA(2).fit_transform([model[x] for x in words])
for (x, y), word in zip(vecs, words):
    plt.scatter(x,y)
    plt.text(x+0.02, y+0.02, word)
plt.show()

for word, similarity in model.most_similar("programming", topn=5):
    print(word, similarity)
\`\`\``
            }
        },
        program3: {
            title: "Program 3: Custom Word2Vec Model",
            v1: {
                content: `# Custom Word2Vec Model (Version 1)

This program demonstrates training a custom Word2Vec model on a small domain-specific corpus.

\`\`\`python
from gensim.models import Word2Vec

# Custom Word2Vec model
def cw(corpus):
    model = Word2Vec(
        sentences=corpus,
        vector_size=50,  # Dimensionality of word vectors
        window=5,        # Context window size
        min_count=1,     # Minimum frequency for a word to be considered
        workers=4,       # Number of worker threads
        epochs=10,       # Number of training epochs
    )
    return model

# Analyze trained embeddings
def anal(model, word):
    sw = model.wv.most_similar(word, topn=5)
    for w, s in sw:
        print(w, s)

# Example domain-specific dataset (medical/legal/etc.)
corpus = [
    "The patient was prescribed antibiotics to treat the infection.".split(),
    "The court ruled in favor of the defendant after reviewing the evidence.".split(),
    "Diagnosis of diabetes mellitus requires specific blood tests.".split(),
    "The legal contract must be signed in the presence of a witness.".split(),
    "Symptoms of the disease include fever, cough, and fatigue.".split(),
]

model = cw(corpus)
print("Analysis for word patient")
anal(model, "patient")
print("Analysis for word court")
anal(model, "court")
\`\`\``
            },
            v2: {
                content: `# Custom Word2Vec Model (Version 2)

This program demonstrates an alternative approach to training a custom Word2Vec model.

\`\`\`python
from gensim.models import Word2Vec

corpus=[
    "The patient was prescribed antibiotics to treat the infection.".split(),
    "The court ruled in favor of the defendant after reviewing the evidence.".split(),
    "Diagnosis of diabetes mellitus requires specific blood tests.".split(),
    "The legal contract must be signed in the presence of a witness.".split(),
    "Symptoms of the disease include fever, cough, and fatigue.".split(),
    "The patient was advised to follow a strict diet and exercise regimen.".split(),
    "The judge delivered a verdict based on the presented facts.".split(),
    "Early detection of cancer can significantly improve treatment outcomes.".split(),
    "The lawyer prepared the case meticulously for the upcoming trial.".split(),
]

model= Word2Vec(corpus, vector_size=50, epochs=10, min_count=1)

for word in ["patient", "court"]:
    print(f"Similar words to '{word}':")
    for word, similarity in model.wv.most_similar(word, topn=5):
        print(f"{word}: {similarity}")
    print()
\`\`\``
            }
        },
        program4: {
            title: "Program 4: Contextual Word Enrichment",
            v1: {
                content: `# Contextual Word Enrichment (Version 1)

This program demonstrates how to enrich prompts with contextually relevant words and compare generation results.

\`\`\`python
from gensim.downloader import load
import torch
from transformers import pipeline

# Load pre-trained word embeddings (GloVe)
model = load("glove-wiki-gigaword-50")
torch.manual_seed(42)

# Define contextually relevant word enrichment
def enrich(prompt):
    ep = ""# Start with the original prompt
    words = prompt.split() # Split the prompt into words
    for word in words:
        sw = model.most_similar(word, topn=3)
        print("Test Data\\n",sw)
        enw=[]
        for s,w in sw:
            enw.append(s)
        ep+=" " + " ".join(enw)
    return ep

# Example prompt to be enriched
op = "lung cancer"
ep = enrich(op)

# Display the results
print("Original Prompt:", op)
print("Enriched Prompt:", ep)
generator = pipeline("text-generation", model="gpt2")#, tokenizer="gpt2")
response = generator(op, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)#top_p=0.95, temperature=0.7)
print("\\n\\nPrompt response\\n",response[0]["generated_text"])
response = generator(ep, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2) #top_p=0.95, temperature=0.7)
print("\\n\\nEnriched prompt response\\n",response[0]["generated_text"])
\`\`\``
            },
            v2: {
                content: `# Contextual Word Enrichment (Version 2)

This program demonstrates an alternative approach to enriching prompts with contextually relevant words.

\`\`\`python
from gensim.downloader import load
from transformers import pipeline

model = load("glove-wiki-gigaword-50")

# Enrich the input prompt with similar words
def enrich(prompt):
    sw = prompt.split()
    for word in prompt.split():
            sw += [w for w, _ in model.most_similar(word, topn=3)]
    return " ".join(sw)

op = "lung cancer"
ep = enrich(op)

print("Original:", op)
print("Enriched:", ep)

gen = pipeline("text-generation", model="gpt2")

print("\nOriginal Response:\n")
print(gen(op, max_length=100)[0]["generated_text"])

print("\nEnriched Response:\n")
print(gen(ep)[0]["generated_text"])
\`\`\``
            }
        },
        program5: {
            title: "Program 5: Paragraph Generation with Word Embeddings",
            v1: {
                content: `# Paragraph Generation with Word Embeddings (Version 1)

This program demonstrates generating a meaningful paragraph using similar words from word embeddings.

\`\`\`python
from gensim.downloader import load
import random

# Load the pre-trained GloVe model
print("Loading pre-trained GloVe model (50 dimensions)...")
model = load("glove-wiki-gigaword-50")
print(model)
print("Model loaded successfully!")

# Function to construct a meaningful paragraph
def create_paragraph(iw, sws):
    paragraph = f"The topic of {iw} is fascinating, often linked to terms like\\n"
    random.shuffle(sws)  # Shuffle to add variety
    for word in sws:
        paragraph += str(word) + ", "
    paragraph = paragraph.rstrip(", ") + "."
    return paragraph

iw = "cricket"
sws = model.most_similar(iw, topn=50)
words = [word for word, s in sws]
paragraph = create_paragraph(iw, words)
print(paragraph)
\`\`\``
            },
            v2: {
                content: `# Paragraph Generation with Word Embeddings (Version 2)

This program demonstrates an alternative approach to paragraph generation using word embeddings.

\`\`\`python
from gensim.downloader import load
import random

model = load("glove-wiki-gigaword-50")
iw = "cricket"
words = [w for w, _ in model.most_similar(iw, topn=50)]
random.shuffle(words)
para = f"The topic of {iw} is fascinating, often linked to terms like {', '.join(words)}."
print(para)
\`\`\``
            }
        },
        program6: {
            title: "Program 6: Sentiment Analysis",
            v1: {
                content: `# Sentiment Analysis (Version 1)

This program demonstrates sentiment analysis on customer feedback using a pre-trained model.

\`\`\`python
from transformers import pipeline

# Specify the model explicitly
sentiment_analyzer = pipeline(
    "sentiment-analysis", 
    model="distilbert/distilbert-base-uncased-finetuned-sst-2-english"
)

customer_feedback = [
    "The product is amazing! I love it!",
    "Terrible service, I am very disappointed.",
    "This is a great experience, I will buy again.",
    "Worst purchase I've ever made. Completely dissatisfied.",
    "I'm happy with the quality, but the delivery was delayed."
]

for feedback in customer_feedback:
    sentiment_result = sentiment_analyzer(feedback)
    sentiment_label = sentiment_result[0]['label']
    sentiment_score = sentiment_result[0]['score']
    
    # Display sentiment results
    print(f"Feedback is: {feedback}")
    print(f"Sentiment is: {sentiment_label} (Confidence: {sentiment_score:.2f})\\n")
\`\`\``
            },
            v2: {
                content: `# Sentiment Analysis (Version 2)

This program demonstrates an alternative approach to sentiment analysis using a different model.

\`\`\`python
from transformers import pipeline

analyze = pipeline("sentiment-analysis", model="distilbert/distilbert-base-uncased-finetuned-sst-2-english")

feedbacks = [
    "The product is amazing! I love it!",
    "Terrible service, I am very disappointed.",
    "This is a great experience, I will buy again.",
    "Worst purchase I’ve ever made. Completely dissatisfied.",
    "I'm happy with the quality, but the delivery was delayed."
]

for fb in feedbacks:
    result = analyze(fb)[0]
    print(f"Feedback: {fb}\nSentiment: {result['label']} (Confidence: {result['score']:.2f})\n")
\`\`\``
            }
        },
        program7: {
            title: "Program 7: Text Summarization",
            v1: {
                content: `# Text Summarization (Version 1)

This program demonstrates automatic text summarization using a pre-trained model.

\`\`\`python
from transformers import pipeline

# Specify the model explicitly
summarizer = pipeline(
    "summarization", 
    model="facebook/bart-large-cnn"  # You can replace this with any other summarization model if needed
)

# Function to summarize a given passage
def summarize_text(text):
    # Summarizing the text using the pipeline
    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)
    return summary[0]['summary_text']

text = """
Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. 
The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable. 
NLP techniques are used in many applications, such as speech recognition, sentiment analysis, machine translation, and chatbot functionality. 
Machine learning algorithms play a significant role in NLP, as they help computers to learn from vast amounts of language data and improve their ability to process and generate text. 
However, NLP still faces many challenges, such as handling ambiguity, understanding context, and processing complex linguistic structures. 
Advances in NLP have been driven by deep learning models, such as transformers, which have significantly improved the performance of many NLP tasks.
"""

# Get the summarized text
summarized_text = summarize_text(text)

# Display the summarized text
print("Original Text:\\n", text)
print("\\nSummarized Text:\\n", summarized_text)
\`\`\``
            },
            v2: {
                content: `# Text Summarization (Version 2)

This program demonstrates an alternative approach to text summarization using a different technique.

\`\`\`python
# Program 7 Version 2 placeholder
# Alternative Text Summarization
\`\`\``
            }
        },
        program8: {
            title: "Program 8: Institution Details Extractor",
            v1: {
                content: `# Institution Details Extractor (Version 1)

This program demonstrates extracting information about institutions from Wikipedia using Pydantic.

\`\`\`python
from pydantic import BaseModel
import wikipediaapi

# Define the Pydantic Schema
class InstitutionDetails(BaseModel):
    name: str
    founder: str
    founded: str
    branches: str
    employees: str
    summary: str

# Helper function to extract info based on keyword
def extract_info(content, keyword):
    for line in content.split('\\n'):
        if keyword in line.lower():
            return line.strip()
    return "Not available"

# Function to Fetch and Extract Details from Wikipedia
def fetch(institution_name):
    user_agent = "InstitutionInfoFetcher/1.0 (https://example.com; contact@example.com)"
    wiki = wikipediaapi.Wikipedia('en', headers={"User-Agent": user_agent})
    page = wiki.page(institution_name)

    if not page.exists():
        raise ValueError(f"No Wikipedia page found for '{institution_name}'")

    content = page.text

    founder = extract_info(content, "founder")
    founded = extract_info(content, "founded") or extract_info(content, "established")
    branches = extract_info(content, "branch")
    employees = extract_info(content, "employee")
    summary = "\\n".join(content.split('\\n')[:4])

    return InstitutionDetails(
        name=institution_name,
        founder=founder,
        founded=founded,
        branches=branches,
        employees=employees,
        summary=summary
    )


# Run the program
details = fetch("PESITM")
print("\\nExtracted Institution Details:")
print(details.model_dump_json(indent=4))
\`\`\``
            },
            v2: {
                content: `# Institution Details Extractor (Version 2)

This program demonstrates an alternative approach to extracting institution details from Wikipedia.

\`\`\`python
from pydantic import BaseModel
import wikipediaapi

class Info(BaseModel):
    name: str
    founder: str
    founded: str
    branches: str
    employees: str
    summary: str

def get_line(text, keyword):
    return next((line for line in text.split('\n') if keyword in line.lower()), "Not available")

def fetch(name):
    wiki = wikipediaapi.Wikipedia(
        language='en',
        headers={"User-Agent": "MySimpleFetcher/1.0 (contact@example.com)"}
    )
    page = wiki.page(name)
    if not page.exists():
        raise ValueError(f"No page found for '{name}'")

    txt = page.text
    return Info(
        name=name,
        founder=get_line(txt, "founder"),
        founded=get_line(txt, "founded") or get_line(txt, "established"),
        branches=get_line(txt, "branch"),
        employees=get_line(txt, "employee"),
        summary="\n".join(txt.split('\n')[:3])
    )

# Run it
print(fetch("VTU").model_dump_json(indent=2))

\`\`\``
            }
        },
        program9: {
            title: "Program 9: IPC PDF Chatbot",
            v1: {
                content: `# IPC PDF Chatbot (Version 1)

This program demonstrates a simple chatbot that can search and retrieve information from an IPC PDF document.

\`\`\`python
import fitz  # PyMuPDF

# Step 1: Extract Text from IPC PDF
def extract(file):
    text = ""
    with fitz.open(file) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

# Step 2: Search for Relevant Sections in IPC
def search(query, ipc):
    query = query.lower()
    lines = ipc.split("\\n")
    results=[]
    for line in lines:
        if query in line.lower():
           results.append(line) 
    #results = [line for line in lines if query in line.lower()]
    if results:
        return results
    else:  
        return ["No relevant section found."]   
    #return results if results else ["No relevant section found."]

# Step 3: Main Chatbot Function
def chatbot():
    print("Loading IPC document...")
    ipc = extract("IPC.pdf")
    while True:
        query = input("Ask a question about the IPC: ")
        if query.lower() == "exit":
            print("Goodbye!")
            break

        results = search(query, ipc)
        print("\\n".join(results))
        print("-" * 50)

chatbot()
\`\`\``
            },
            v2: {
                content: `# IPC PDF Chatbot (Version 2)

This program demonstrates an alternative approach to creating a chatbot for IPC PDF documents.

\`\`\`python
import fitz  # PyMuPDF

def extract(file):
    with fitz.open(file) as pdf:
        return "".join(page.get_text() for page in pdf)

def search(query, text):
    query = query.lower()
    lines = text.split('\n')
    results = []
    for line in lines:
        if query in line.lower():
            results.append(line)

def chatbot():
    print("Loading IPC...")
    ipc = extract("IPC.pdf")
    while True:
        query = input("Ask about IPC (type 'exit' to quit): ").strip()
        if query.lower() == "exit":
            print("Goodbye!")
            break
        print("\n".join(search(query, ipc)))
        print("-" * 50)

chatbot()
\`\`\``
            }
        }
    };
    
    // Initialize with program1
    displayProgram(currentProgramIndex, currentVersion);
    
    // Functions
    function displayProgram(index, version) {
        // Convert index to program ID
        const programId = `program${index + 1}`;
        const program = programs[programId];
        const versionKey = `v${version}`;
        
        // Update the program indicator
        programIndicator.textContent = `Program ${index + 1}/${totalPrograms} (V${version})`;
        
        // Get the raw content and parse for display
        const rawContent = program[versionKey].content;
        
        // Split content into parts (description and code)
        const codeMatch = rawContent.match(/```python\n([\s\S]*?)```/);
        
        if (codeMatch) {
            // Extract the code and description
            const code = codeMatch[1];
            const description = rawContent.substring(0, codeMatch.index).trim();
            
            // Store raw code for copy functionality
            rawCodeContent = code;
            
            // Create and display content as plain text
            const contentText = description + "\n\n" + code;
            
            // Set content directly to the pre element
            programContent.textContent = contentText;
        } else {
            // Just display the raw content if there's no code block
            programContent.textContent = rawContent;
        }
    }
    
    // Double click to toggle between versions
    programContent.addEventListener('dblclick', function() {
        // Toggle between version 1 and 2
        currentVersion = currentVersion === 1 ? 2 : 1;
        displayProgram(currentProgramIndex, currentVersion);
    });
    
    // Arrow key navigation
    document.addEventListener('keydown', function(event) {
        // Handle arrow keys
        if (event.key === 'ArrowRight' || event.key === 'ArrowDown') {
            // Next program
            currentProgramIndex = (currentProgramIndex + 1) % totalPrograms;
            displayProgram(currentProgramIndex, currentVersion);
            event.preventDefault();
        } else if (event.key === 'ArrowLeft' || event.key === 'ArrowUp') {
            // Previous program
            currentProgramIndex = (currentProgramIndex - 1 + totalPrograms) % totalPrograms;
            displayProgram(currentProgramIndex, currentVersion);
            event.preventDefault();
        }
    });
    
    // Swipe gesture detection for mobile
    document.addEventListener('touchstart', function(event) {
        touchStartX = event.changedTouches[0].screenX;
    }, false);
    
    document.addEventListener('touchend', function(event) {
        touchEndX = event.changedTouches[0].screenX;
        handleSwipe();
    }, false);
    
    function handleSwipe() {
        const swipeThreshold = 50; // Minimum distance for a swipe
        const swipeDistance = touchEndX - touchStartX;
        
        if (swipeDistance > swipeThreshold) {
            // Swipe right - go to previous program
            currentProgramIndex = (currentProgramIndex - 1 + totalPrograms) % totalPrograms;
            displayProgram(currentProgramIndex, currentVersion);
        } else if (swipeDistance < -swipeThreshold) {
            // Swipe left - go to next program
            currentProgramIndex = (currentProgramIndex + 1) % totalPrograms;
            displayProgram(currentProgramIndex, currentVersion);
        }
    }
    
    // Copy button functionality
    copyBtn.addEventListener('click', function() {
        // Use the stored raw code for copying
        navigator.clipboard.writeText(rawCodeContent).then(function() {
            // Visual feedback for copied state
            copyBtn.classList.add("copied");
            
            // Reset after 2 seconds
            setTimeout(function() {
                copyBtn.classList.remove("copied");
            }, 2000);
        });
    });
}); 

    </script>
</body>
</html> 
